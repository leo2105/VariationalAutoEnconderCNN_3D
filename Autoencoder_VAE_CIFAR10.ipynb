{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "\n",
    "# %matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = unpickle('/home/computadora1/Escritorio/VAE/cifar-10-batches-py/data_batch_1')\n",
    "batch2 = unpickle('/home/computadora1/Escritorio/VAE/cifar-10-batches-py/data_batch_2')\n",
    "batch3 = unpickle('/home/computadora1/Escritorio/VAE/cifar-10-batches-py/data_batch_3')\n",
    "batch4 = unpickle('/home/computadora1/Escritorio/VAE/cifar-10-batches-py/data_batch_4')\n",
    "batch5 = unpickle('/home/computadora1/Escritorio/VAE/cifar-10-batches-py/data_batch_5')\n",
    "\n",
    "for a in batch1:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = batch5[b'data']\n",
    "for arr in dataset:\n",
    "#     img_r = arr[:1024].reshape((32,32))\n",
    "#     img_g = arr[1024:-1024].reshape((32,32))\n",
    "#     img_b = arr[-1024:].reshape((32,32))\n",
    "#     img_rgb = cv2.merge([img_r, img_g, img_b])\n",
    "    images.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dataset = np.ndarray(shape=(50000,3072), dtype=np.uint8)\n",
    "Dataset[:] = images[:]\n",
    "#Dataset = np.ndarray((images))\n",
    "#print(Dataset.shape)\n",
    "Dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure, imshow, axis\n",
    "\n",
    "def mnist_grid(X):\n",
    "    X = X.reshape([-1,3072])\n",
    "    num_images = X.shape[0]\n",
    "    fig = figure()\n",
    "    fig.set_size_inches(10,10)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        a = fig.add_subplot(1, num_images, i+1)\n",
    "        img_r = X[i,:1024].reshape((32,32))\n",
    "        img_g = X[i,1024:-1024].reshape((32,32))\n",
    "        img_b = X[i,-1024:].reshape((32,32))\n",
    "        img_rgb = cv2.merge([img_r, img_g, img_b])\n",
    "        imshow(img_rgb)\n",
    "        axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Dataset[:40000]\n",
    "X_valid = Dataset[-10000:]\n",
    "mnist_grid(X_train[:10])\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = X_train.shape[1] # 3072\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 256\n",
    "n_hidden3 = 128\n",
    "n_hidden4 = 32\n",
    "n_hidden5 = n_hidden3\n",
    "n_hidden6 = n_hidden2\n",
    "n_hidden7 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden1 = tf.layers.dense(x, n_hidden1, activation=tf.nn.elu)\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu)\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.elu)\n",
    "hidden4_mean = tf.layers.dense(hidden3, n_hidden4, activation=None)\n",
    "hidden4_sigma = tf.layers.dense(hidden3, n_hidden4, activation=None)\n",
    "noise = tf.random_normal(tf.shape(hidden4_sigma), dtype=tf.float32)\n",
    "hidden4 = hidden4_mean + hidden4_sigma * noise\n",
    "hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.elu)\n",
    "hidden6 = tf.layers.dense(hidden5, n_hidden6, activation=tf.nn.elu)\n",
    "hidden7 = tf.layers.dense(hidden6, n_hidden7, activation=tf.nn.elu)\n",
    "logits = tf.layers.dense(hidden7, n_outputs, activation=None)\n",
    "outputs = tf.sigmoid(logits)\n",
    "\n",
    "# MSE loss\n",
    "def lossFunction(X):\n",
    "    mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(X - outputs), axis=1))\n",
    "    return mse_loss\n",
    "\n",
    "#\n",
    "# Funcion de perdida (MSE) y entrenamiento\n",
    "#\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=logits)\n",
    "xentropy_loss = tf.reduce_sum(xentropy)\n",
    "eps = 1e-10\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "    # centre en 0 y los demas los centre a 1\n",
    "    tf.square(hidden4_sigma) + tf.square(hidden4_mean) - 1 - tf.log(eps + tf.square(hidden4_sigma))\n",
    ") \n",
    "\n",
    "loss = xentropy_loss + latent_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "loss_training = lossFunction(X_train) \n",
    "loss_validation = lossFunction(X_valid)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 101\n",
    "\n",
    "# points = []\n",
    "# points_train = []\n",
    "# points_valid = []\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# plt.ion()\n",
    "\n",
    "# fig.show()\n",
    "# fig.canvas.draw()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for iteration in range(n_iterations):\n",
    "        \n",
    "        _, mse_train = sess.run([training_op, loss_training], feed_dict={x: X_train})\n",
    "#        mse_valid = sess.run([loss_validation],feed_dict={x:X_valid})\n",
    "                \n",
    "#         points_train.append(mse_train)\n",
    "#         points_valid.append(mse_valid)\n",
    "        \n",
    "#         points = np.arange(iteration + 1)\n",
    "        \n",
    "#         ax.clear()\n",
    "        \n",
    "#         ax.plot(points, points_train, color='blue')\n",
    "#         ax.plot(points, points_valid, color='red')\n",
    "#         fig.canvas.draw()\n",
    "        #time.sleep(1.0)\n",
    "        #plt.show()\n",
    "        \n",
    "        if iteration % 50 == 0:\n",
    "            #mse_valid = sess.run([loss_validation],feed_dict={x:X_valid})\n",
    "\n",
    "            print('Iteracion: {:04d}'.format(iteration))\n",
    "            print('MSE_train: {:.1f}'.format(mse_train))\n",
    "            #print('MSE_valid: {:.9f}'.format(mse_valid[0]))\n",
    "            ##print(mse_valid)\n",
    "            ##print('Iteraci√≥n: {:04d}, MSE: {:.9f}'.format(iteration, mse))\n",
    "            R = sess.run(outputs, feed_dict={x:X_train[:10]})\n",
    "            mnist_grid(R)\n",
    "            \n",
    "    saver.save(sess, \"./VAE_2.ckpt\")\n",
    "    \n",
    "    #R = sess.run(outputs, feed_dict={x:X_valid[:10]})\n",
    "    #mnist_grid(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_digits = 10\n",
    "random_codings = np.random.normal(size=[n_digits, n_hidden4])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  saver.restore(sess, \"./VAE_2.ckpt\")\n",
    "  generated_images = sess.run(outputs, feed_dict={hidden4: random_codings})\n",
    "\n",
    "mnist_grid(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_valid[-10:]\n",
    "\n",
    "points = np.arange(len(points_train))\n",
    "plt.title('Reconstruction MNIST images 2D - VAE Dense Layers')\n",
    "plt.ylabel('MSE - Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.plot(points, points_train, label='train')\n",
    "plt.plot(points, points_valid, label='test')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
